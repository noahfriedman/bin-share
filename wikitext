#!/usr/bin/env python
# wikitext -- retrieve wikitext from a wikipedia page title
# Author: Noah Friedman <friedman@splode.com>
# Created: 2025-03-16
# Public domain

# Commentary:
# Code:

import argparse
import json
import re
import sys

import requests
from   requests.exceptions import RequestException

class WikiSession( requests.Session ):
    default_api_url = 'https://en.wikipedia.org/w/api.php'
    default_param   = { 'action' : 'parse',
                        'format' : 'json',
                        'prop'   : 'wikitext',
                        'page'   : None, }


    def __init__( self, **kwargs ):
        super().__init__()
        self.param   = self.default_param.copy()
        self.api_url = self.default_api_url

        for k, v in kwargs.items():
            if k == 'page':
                if isinstance( v, list ):
                    self.param[ k ] = '_'.join( v )
                elif isinstance( v, str ):
                    self.param[ k ] = v.replace( ' ', '_' )
            elif k == 'api_url':
                if v:
                    setattr( self, k, v)
            elif k in self.param:
                self.param[ k ] = v
            else:
                pass  # ignore anything else


    def fetch( self, **kwargs ):
        param = self.param.copy()
        if kwargs:
            param.update( kwargs )

        resp = self.get( self.api_url, params=param )
        if resp.status_code != 200:
            msg = '{} {}'.format( resp.status_code, resp.reason )
            raise RequestException( msg )
        return json.loads( resp.text )


class MyFormatter( argparse.HelpFormatter ):
    def _fill_text( self, text, width=75, indent=0 ):
        width     = min( 75, width )
        text      = text.expandtabs( 8 )
        text      = text.replace( '\r', '' ) # CRLF -> LF
        paragraph = text.split( '\n\n' )     # Split into separate chunks.
        re_ll = re.compile( r'(.{1,%s})(?:[ \t]+|$)' % width, flags=re.M )
        filled = []
        for para in paragraph:
            if re.match( r'^\s*[#$]', para ):  # don't fill code examples
                filled.append( para )
                continue
            # replace LF and trailing/leading whitespace with single space
            para = re.sub( r'[ \t]*\n[ \t]*', ' ', para.strip(), flags=re.M )
            # split into lines no longer than width but only at whitespace.
            para = re.sub( re_ll, '\\1\n', para )
            # but remove final newline
            para = re.sub( '\n+$', '', para, flags=re.M )
            filled.append( para )
        text = str.join( '\n\n', filled ) # rejoin paragraphs at the end.
        if indent:
            spc  = (' ' * indent)
            text = re.sub( '^', spc, text, flags=re.M )
        return text

def get_args():
    help = {
        'json'    : 'format output as JSON',
        'section' : 'case-insensitive section number, name, or regex',
        'api-url' : 'API url (default is for wikipedia.com)',
        'title'   : 'page title or component words',

        'epilog'  : """
                    The SECTION argument can be an index starting from 1,
                    or a negative index from the end of the sections,
                    e.g. '-1' means the last section.
                    If SECTION begins and ends with `/', the enclosed
                    sequence is treated as a regular expression.

                    If --json argument is specified, output consists of
                    the wikitext plus an array of section definitions.
                    The --section argument is ignored.
                    """ }

    p = argparse.ArgumentParser( formatter_class=MyFormatter,
                                 epilog=help[ 'epilog' ] )
    p.add_argument( '-j', '--json',    help=help['json'],    action='store_true', default=False )
    p.add_argument( '-s', '--section', help=help['section'] )
    p.add_argument( '-u', '--api-url', help=help['api-url'] )
    p.add_argument( 'title',           help=help['title'],   nargs='+' )
    return p.parse_args()


def main():
    args    = get_args()
    session = WikiSession( **args.__dict__, page=args.title )
    data    = session.fetch()
    text    = data['parse']['wikitext' ]['*']

    if args.json:
        data    = session.fetch( prop='sections' )
        section = data['parse']['sections']
        combo = { 'wikitext': text,
                  'sections': section, }
        print( json.dumps( combo ))

    elif not ( args.section is None or args.section == '' ):
        # n.b. the API supports a 'section' field, but it's limited to
        # positive integer values only, and it's actually slower to
        # retrieve the section text for average-size pages.  It's actually
        # faster to retrieve the entire page and the section list, and do
        # the processing locally.  Plus, we can do more flexible matching.
        data    = session.fetch( prop='sections' )
        section = data['parse']['sections']
        # Should be sorted already, but don't trust that
        section.sort( key=lambda elt: elt['byteoffset'] )

        wantre   = None
        wantsect = args.section.lower()
        if wantsect[0] == wantsect[-1] == '/':
            wantre = re.compile( wantsect[1:-1], flags=re.IGNORECASE )
        else:
            try:
                negno = int( wantsect )
                if negno < 0:
                    numsects = len( section )
                    newno = numsects + negno
                    if numsects > newno > 0:
                        wantsect = str( newno+1 )
            except ValueError:
                pass

        search_fields = ('line', 'anchor', 'index', 'number')
        found = []
        for i in range( 0, len( section )):
            elt        = section[ i ]
            elt_fields = [ elt[ name ].lower() for name in search_fields ]

            ismatch = False
            if wantre:
                if any( wantre.match( field ) for field in elt_fields ):
                    ismatch = True
            elif wantsect in elt_fields:
                ismatch = True

            if ismatch:
                beg = elt['byteoffset']
                try:
                    end = section[ i+1 ][ 'byteoffset' ]
                except IndexError:
                    end = None
                found.append( slice( beg, end ))
        if found:
            foundtext = ''.join( text[ r ] for r in found )
            print( foundtext.rstrip() )
        else:
            print( 'Section not found', file=sys.stderr )
            return 1
    else:
        print( text )


if __name__ == '__main__':
    sys.exit( main() )

# eof
