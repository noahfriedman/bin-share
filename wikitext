#!/usr/bin/env python
# wikitext -- retrieve wikitext from a wikipedia page title
# Author: Noah Friedman <friedman@splode.com>
# Created: 2025-03-16
# Public domain

# Commentary:
# Code:

import argparse
import json
import re
import sys

import requests
from   requests.exceptions import RequestException

class WikiSession( requests.Session ):
    default_api_url = 'https://en.wikipedia.org/w/api.php'
    default_param   = { 'action' : 'parse',
                        'format' : 'json',
                        'prop'   : 'wikitext',
                        'page'   : None, }


    def __init__( self, **kwargs ):
        super().__init__()
        self.param   = self.default_param.copy()
        self.api_url = self.default_api_url

        for k, v in kwargs.items():
            if k == 'page':
                if isinstance( v, list ):
                    self.param[ k ] = '_'.join( v )
                elif isinstance( v, str ):
                    self.param[ k ] = v.replace( ' ', '_' )
            elif k == 'api_url':
                if v:
                    setattr( self, k, v)
            elif k in self.param:
                self.param[ k ] = v
            else:
                pass  # ignore anything else


    def fetch( self, **kwargs ):
        param = self.param.copy()
        if kwargs:
            param.update( kwargs )

        resp = self.get( self.api_url, params=param )
        if resp.status_code != 200:
            msg = '{} {}'.format( resp.status_code, resp.reason )
            raise RequestException( msg )
        return json.loads( resp.text )


class MyFormatter( argparse.HelpFormatter ):
    def _fill_text( self, text, width=75, indent=0 ):
        width     = min( 75, width )
        text      = text.expandtabs( 8 )
        text      = text.replace( '\r', '' ) # CRLF -> LF
        paragraph = text.split( '\n\n' )     # Split into separate chunks.
        re_ll = re.compile( r'(.{1,%s})(?:[ \t]+|$)' % width, flags=re.M )
        filled = []
        for para in paragraph:
            if re.match( r'^\s*[#$]', para ):  # don't fill code examples
                filled.append( para )
                continue
            # replace LF and trailing/leading whitespace with single space
            para = re.sub( r'[ \t]*\n[ \t]*', ' ', para.strip(), flags=re.M )
            # split into lines no longer than width but only at whitespace.
            para = re.sub( re_ll, '\\1\n', para )
            # but remove final newline
            para = re.sub( '\n+$', '', para, flags=re.M )
            filled.append( para )
        text = str.join( '\n\n', filled ) # rejoin paragraphs at the end.
        if indent:
            spc  = (' ' * indent)
            text = re.sub( '^', spc, text, flags=re.M )
        return text

def get_args():
    help = {
        'json'    : 'Format output as JSON',
        'section' : 'Section name or number',
        'api-url' : 'API url (default is for wikipedia.com)',
        'title'   : 'Wikipedia page title or component words',

        'epilog'  : """
                    In addition to titles or anchors,
                    the SECTION argument can be an index starting from 1,
                    or a negative index from the end of the sections,
                    e.g. '-1' means the last section.

                    If --json argument is specified, output consists of
                    the wikitext plus an array of section definitions.
                    The --section argument is ignored.
                    """ }

    p = argparse.ArgumentParser( formatter_class=MyFormatter,
                                 epilog=help[ 'epilog' ] )
    p.add_argument( '-j', '--json',    help=help['json'],    action='store_true', default=False )
    p.add_argument( '-s', '--section', help=help['section'] )
    p.add_argument( '-u', '--api-url', help=help['api-url'] )
    p.add_argument( 'title',           help=help['title'],   nargs='+' )
    return p.parse_args()


def main():
    args    = get_args()
    session = WikiSession( **args.__dict__, page=args.title )
    data    = session.fetch()
    text    = data['parse']['wikitext' ]['*']

    if args.json:
        data    = session.fetch( prop='sections' )
        section = data['parse']['sections']
        combo = { 'wikitext': text,
                  'sections': section, }
        print( json.dumps( combo ))
    elif args.section:
        data    = session.fetch( prop='sections' )
        section = data['parse']['sections']
        # Should be sorted already, but don't trust that
        section.sort( key=lambda elt: elt['byteoffset'] )

        wantsect = args.section.lower()
        try:
            negno = int( wantsect )
            if negno < 0:
                numsects = len( section )
                newno = numsects + negno
                if numsects > newno > 0:
                    wantsect = str( newno+1 )
        except ValueError:
            pass

        for i in range( 0, len( section )):
            elt = section[ i ]
            if wantsect in ( elt['line'].lower(),
                             elt['anchor'].lower(),
                             str( elt['index'] )):
                beg = elt['byteoffset']
                try:
                    end = section[ i+1 ][ 'byteoffset' ]
                except IndexError:
                    end = None
                print( text[ beg : end ] )
                break
        else:
            print( 'Section not found', file=sys.stderr )
            return 1
    else:
        print( text )


if __name__ == '__main__':
    sys.exit( main() )

# eof
